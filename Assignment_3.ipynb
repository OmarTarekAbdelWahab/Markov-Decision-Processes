{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmarTarekAbdelWahab/Markov-Decision-Processes/blob/main/Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#**Assignment 3**\n",
        "---\n",
        "##**Markov Decision Processes**\n",
        "\n",
        "\n",
        "\n",
        "*   Omar Tarek Abdelwahab       20010998\n",
        "*   Abdelrahman Elsayed Mohamed 20010795\n",
        "*   Marwan Mostafa Abd El-Kader 20011867\n",
        "*   Ali Mones Abd El-Mohsen     20010946\n",
        "\n"
      ],
      "metadata": {
        "id": "JNjiND2MwV-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROW_SIZE, COL_SIZE = 3, 3\n",
        "GAMMA = .99"
      ],
      "metadata": {
        "id": "6LuXuVwN7ogb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOVES**: array of possible moves at any cell\n",
        "\n",
        "**directions**: used to print the policies in a prettier way"
      ],
      "metadata": {
        "id": "TPf-4C607XYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MOVES = [(0,1),(0,-1),(1,0),(-1,0)]\n",
        "directions = {\n",
        "      (0,0):\"Term\",\n",
        "      (0,1):\"right\",\n",
        "      (0,-1):\"left\",\n",
        "      (1,0):\"down\",\n",
        "      (-1,0):\"up\"\n",
        "}"
      ],
      "metadata": {
        "id": "hlBCpA4x8aY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Choose epsiolon value to be 1e-10\n",
        "to end the iteration"
      ],
      "metadata": {
        "id": "HmzULAr4x5gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPSILON = 0.0000000001"
      ],
      "metadata": {
        "id": "r1OaPqJa9ISo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the next child states of the current state and current action.\n",
        "return a dictionary with the state as key, and the probability of reaching that state as the value.\n"
      ],
      "metadata": {
        "id": "PI2fti_7yTP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getNextVerMove(row, a_x):\n",
        "  return min(max(row + a_x, 0), ROW_SIZE-1)\n",
        "def getNextHozMove(col, a_y):\n",
        "  return min(max(col + a_y, 0), COL_SIZE-1)\n",
        "def addToDict(dic, v1, v2, val):\n",
        "  if (v1, v2) not in dic:\n",
        "    dic[(v1,v2)] = 0\n",
        "  dic[(v1,v2)] += val\n",
        "def getNextStates(row, col, action):\n",
        "  a_x, a_y = action[0], action[1]\n",
        "  nextStates = {}\n",
        "  if a_y == 0:\n",
        "    nextStates[(getNextVerMove(row, a_x), col)] = .8\n",
        "    addToDict(nextStates, row,getNextHozMove(col, 1), .1)\n",
        "    addToDict(nextStates, row,getNextHozMove(col, -1), .1)\n",
        "  else:\n",
        "    nextStates[(row, getNextHozMove(col, a_y))] = .8\n",
        "    addToDict(nextStates, getNextVerMove(row,1), col, .1)\n",
        "    addToDict(nextStates, getNextVerMove(row,-1), col, .1)\n",
        "  return nextStates"
      ],
      "metadata": {
        "id": "q-sJU3Nm974y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to calculate the relative error of the current value of V, copmuted by taking the max difference of the values in the current V array and the previous one.\n"
      ],
      "metadata": {
        "id": "_AOxjYihyr_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculateRelativeError(prev, next):\n",
        "  diff = 0\n",
        "  for i in range(ROW_SIZE):\n",
        "    for j in range(COL_SIZE):\n",
        "      if i==0 and (j==0 or j == 2):\n",
        "        continue\n",
        "      diff = max(diff,abs(prev[i][j] - next[i][j]))\n",
        "  return diff"
      ],
      "metadata": {
        "id": "HUgZ0IdaLCbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_copy(prev, next):\n",
        "  for i in range(ROW_SIZE):\n",
        "    for j in range(COL_SIZE):\n",
        "      prev[i][j] = next[i][j]"
      ],
      "metadata": {
        "id": "jFTl8GR0bqds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to implement the value iteration according to the Bellman equation:\n",
        "> **V(s)=max a: ∑T(s,a,s′)​(R(s,a,s′)+γV(s′))**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "j2p0MEl1zFGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(reward):\n",
        "  V_0 = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "  V_1 = [[0,float('-inf'),0],\n",
        "        [float('-inf'),float('-inf'),float('-inf')],\n",
        "        [float('-inf'),float('-inf'),float('-inf')]\n",
        "          ]\n",
        "  curr_error = 1\n",
        "  cnt = 0\n",
        "  while curr_error > EPSILON:\n",
        "    cnt += 1\n",
        "    for i in range(ROW_SIZE):\n",
        "      for j in range(COL_SIZE):\n",
        "        if i==0 and (j==0 or j == 2):\n",
        "          continue\n",
        "        for move in MOVES:\n",
        "          sum = 0\n",
        "          nextStates = getNextStates(i,j,move)\n",
        "          for nextState in nextStates:\n",
        "            sum += nextStates[nextState]*(reward[nextState[0]][nextState[1]]+GAMMA*V_0[nextState[0]][nextState[1]])\n",
        "          V_1[i][j] = max(V_1[i][j], sum)\n",
        "    curr_error = calculateRelativeError(V_0, V_1)\n",
        "    get_copy(V_0, V_1)\n",
        "  return cnt, V_0"
      ],
      "metadata": {
        "id": "xzUSVthj9YUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function to extract the policy, done once after the value iteration.\n"
      ],
      "metadata": {
        "id": "Y2ZceGJG1YoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_extraction(V, reward):\n",
        "  policy = [[(0,0),(0,0),(0,0)],[(0,0),(0,0),(0,0)],[(0,0),(0,0),(0,0)]]\n",
        "  for i in range(ROW_SIZE):\n",
        "    for j in range(COL_SIZE):\n",
        "      if i==0 and (j==0 or j == 2):\n",
        "        continue\n",
        "      mx = float('-inf')\n",
        "      for move in MOVES:\n",
        "        sum = 0\n",
        "        nextStates = getNextStates(i,j,move)\n",
        "        for nextState in nextStates:\n",
        "          sum += nextStates[nextState]*(reward[nextState[0]][nextState[1]]+GAMMA*V[nextState[0]][nextState[1]])\n",
        "        if sum > mx:\n",
        "          mx = sum\n",
        "          policy[i][j] = move\n",
        "  return policy"
      ],
      "metadata": {
        "id": "LJ5pq3ZTXSNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reset the variables."
      ],
      "metadata": {
        "id": "r35Dk2QW1obr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQlSPlOYta6C",
        "outputId": "1613d77d-7601-437a-df68-7438d7289c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Once deleted, variables cannot be recovered. Proceed (y/[n])? y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "print the states with the policies."
      ],
      "metadata": {
        "id": "KquqyiVu1shb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_policy(policy):\n",
        "  for i in range(ROW_SIZE):\n",
        "    for j in range(COL_SIZE):\n",
        "      print(directions[policy[i][j]], end = \" \")\n",
        "    print(\"\")"
      ],
      "metadata": {
        "id": "9YIvPL5EnZI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the policy iteration, according to the function:\n",
        "> **VΠ(s)=max a: ∑T(s,Π(s),s′)​(R(s,Π(s),s′)+γVΠ(s′))**"
      ],
      "metadata": {
        "id": "5MKbZiY04o9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_iteration(reward, policy):\n",
        "  V_0 = [[0,0,0],[0,0,0],[0,0,0]]\n",
        "  V_1 = [[0, float('-inf'), 0],\n",
        "        [float('-inf'), float('-inf'), float('-inf')],\n",
        "        [float('-inf'), float('-inf'), float('-inf')]]\n",
        "  curr_error = 1\n",
        "  cnt = 0\n",
        "  while curr_error > EPSILON:\n",
        "    cnt += 1\n",
        "    for i in range(ROW_SIZE):\n",
        "      for j in range(COL_SIZE):\n",
        "        if i==0 and (j==0 or j == 2):\n",
        "          continue\n",
        "        move = policy[i][j]\n",
        "        sum = 0\n",
        "        nextStates = getNextStates(i,j,move)\n",
        "        for nextState in nextStates:\n",
        "          sum += nextStates[nextState]*(reward[nextState[0]][nextState[1]]+GAMMA*V_0[nextState[0]][nextState[1]])\n",
        "        V_1[i][j] = sum\n",
        "    curr_error = calculateRelativeError(V_0, V_1)\n",
        "    get_copy(V_0, V_1)\n",
        "  return cnt, V_0"
      ],
      "metadata": {
        "id": "d3BTsbyohaUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "call the policy iteration then the extraction, break when the policy isn't changing anymore.\n"
      ],
      "metadata": {
        "id": "jqtdp8ag65-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation(reward, policy):\n",
        "  cnt = 0\n",
        "  V = []\n",
        "  while True:\n",
        "    cnt+=1\n",
        "    iterations, V = policy_iteration(reward, policy)\n",
        "    new_policy = policy_extraction(V, reward)\n",
        "    if policy == new_policy:\n",
        "      break\n",
        "    policy = new_policy\n",
        "  return cnt, policy, V\n"
      ],
      "metadata": {
        "id": "m2du68gIkgGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call both approaches with\n",
        "\n",
        "\n",
        "```\n",
        "r = [-3, 0, 3, 100]\n",
        "```"
      ],
      "metadata": {
        "id": "3QQBXbAp7FcM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# value iteration:\n",
        "for r in [100, 3, 0, -3]:\n",
        "  temp_policy = [[(0,1),(0,1),(0,1)],[(0,1),(0,1),(0,1)],[(0,1),(0,1),(0,1)]]\n",
        "  print(\"r =\",r)\n",
        "  reward = [[r,-1,10],[-1,-1,-1],[-1,-1,-1]]\n",
        "  iterations, V = value_iteration(reward)\n",
        "  print(\"Finished value iterations in \",iterations, \"iterations\")\n",
        "  print(\"V =\",V)\n",
        "  print(\"policy after the policy extraction:\")\n",
        "  print_policy(policy_extraction(V, reward))\n",
        "  print()\n",
        "  iterations, policy, V = policy_evaluation(reward, temp_policy)\n",
        "  print(\"Finished Policy evaluation in \", iterations, \"iterations\")\n",
        "  print(\"V =\",V)\n",
        "  print(\"Policy:\")\n",
        "  print_policy(policy)\n",
        "  print(end = \"\\n\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rj6LPmZ9vX2v",
        "outputId": "30723b30-cea3-405c-ce24-066f94681372"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "r = 100\n",
            "Finished value iterations in  44 iterations\n",
            "V = [[0, 99.19548076040549, 0], [99.19548076040549, 96.71846631440145, 90.10531413789691], [96.44629713553942, 94.2958884533505, 91.67898973883231]]\n",
            "policy after the policy extraction:\n",
            "Term left Term \n",
            "up left down \n",
            "up left left \n",
            "\n",
            "Finished Policy evaluation in  4 iterations\n",
            "V = [[0, 99.19548076040545, 0], [99.19548076040545, 96.71846631440124, 90.10531413789214], [96.44629713553917, 94.29588845334996, 91.67898973883001]]\n",
            "Policy:\n",
            "Term left Term \n",
            "up left down \n",
            "up left left \n",
            "\n",
            "\n",
            "\n",
            "r = 3\n",
            "Finished value iterations in  39 iterations\n",
            "V = [[0, 9.557514418221283, 0], [6.448045509276803, 8.195156472913931, 9.557514418221283], [5.631127098398129, 6.862738649324498, 8.045463424538513]]\n",
            "policy after the policy extraction:\n",
            "Term right Term \n",
            "right right up \n",
            "right right up \n",
            "\n",
            "Finished Policy evaluation in  3 iterations\n",
            "V = [[0, 9.557514418220777, 0], [6.4480455092711075, 8.195156472911938, 9.557514418220777], [5.63112709838591, 6.8627386493191835, 8.045463424536141]]\n",
            "Policy:\n",
            "Term right Term \n",
            "right right up \n",
            "right right up \n",
            "\n",
            "\n",
            "\n",
            "r = 0\n",
            "Finished value iterations in  39 iterations\n",
            "V = [[0, 9.557514418221036, 0], [6.144746246146492, 8.195156472912961, 9.557514418221036], [5.597801208215017, 6.86273864932191, 8.04546342453736]]\n",
            "policy after the policy extraction:\n",
            "Term right Term \n",
            "right right up \n",
            "right right up \n",
            "\n",
            "Finished Policy evaluation in  3 iterations\n",
            "V = [[0, 9.557514418220777, 0], [6.144746246143568, 8.195156472911938, 9.557514418220777], [5.597801208208745, 6.8627386493191835, 8.045463424536141]]\n",
            "Policy:\n",
            "Term right Term \n",
            "right right up \n",
            "right right up \n",
            "\n",
            "\n",
            "\n",
            "r = -3\n",
            "Finished value iterations in  39 iterations\n",
            "V = [[0, 9.557514418220983, 0], [5.841446983018354, 8.195156472912752, 9.557514418220983], [5.564475318036563, 6.862738649321353, 8.04546342453711]]\n",
            "policy after the policy extraction:\n",
            "Term right Term \n",
            "right right up \n",
            "right right up \n",
            "\n",
            "Finished Policy evaluation in  3 iterations\n",
            "V = [[0, 9.557514418220777, 0], [5.841446983016029, 8.195156472911938, 9.557514418220777], [5.564475318031579, 6.8627386493191835, 8.045463424536141]]\n",
            "Policy:\n",
            "Term right Term \n",
            "right right up \n",
            "right right up \n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With r = 100 we got the policy:\n",
        "\n",
        "```\n",
        "Term left Term\n",
        "up left down\n",
        "up left left\n",
        "```\n",
        "***where Term means a terminal state.***\n",
        "\n",
        "All cells are following the intuitive path to the cell with the highest value ( r ), except for the last cell in the second row with the ( **down** ) move, the reason fot this move is that it wants to avoid the 10% chance to go up to the other terminal state with the value 10, where the game would also end.\n",
        "\n",
        "---\n",
        "\n",
        "With r = 3 we got the policy:\n",
        "```\n",
        "Term right Term\n",
        "right right up\n",
        "right right up\n",
        "```\n",
        "\n",
        "All the cells are trying to reach the topright cell with the highest reward of 10, notice that the first cell in the second row did't move down like the previous example, meaning that the 10% chance to get a reward of 3 is not as bad, and going around the whole grid to reach the 10 goal might not be worth it.\n",
        "\n",
        "---\n",
        "\n",
        "With r = 0 er got the policy:\n",
        "```\n",
        "Term right Term\n",
        "right right up\n",
        "right right up\n",
        "```\n",
        "\n",
        "Which is the same as r = 3.\n",
        "\n",
        "---\n",
        "With r = -3 we got the policy:\n",
        "\n",
        "```\n",
        "Term right Term\n",
        "right right up\n",
        "right right up\n",
        "```\n",
        "\n",
        "Still -3 is not enough to get the policy to avoid it.\n",
        "Using binary search, we found out that -16 is the max value at which it considers to go down and around.\n",
        "```\n",
        "r = -16\n",
        "Term right Term\n",
        "down right up\n",
        "right right up\n",
        "```"
      ],
      "metadata": {
        "id": "TlJy1BLsCISn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}